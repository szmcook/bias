{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit1fe2bb4cf81446c4930ef0f70b991c44",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Classical Implementation\n",
    "In this notebook I have trained a classical (and likely biased) machine learning model on the original dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Useful information for reference:\n",
    "\n",
    "number of instances: 1000 (190 young and 810 aged)\n",
    "\n",
    "labels: 1 is good, 2 is bad\n",
    "\n",
    "A13 == 0 means the individual is young"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 0.1 Read in the data and export it as a CSV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('../german-credit-dataset/german.data-numeric', 'r') as infile:\n",
    "    data_contents = infile.read()    \n",
    "    data_contents = re.sub(r'[ ]+', \",\", data_contents)\n",
    "    data_contents = re.sub(r'^,', \"\", data_contents)\n",
    "    data_contents = re.sub(r'\\n,', \"\\n\", data_contents)\n",
    "    data_contents = re.sub(r',\\n', \"\\n\", data_contents)\n",
    "    # data_contents = re.sub(r'^,|\\n,|,\\n', \"\\n\", data_contents)\n",
    "\n",
    "    with open('../german-credit-dataset/german-numeric.csv', 'w') as outfile:\n",
    "        outfile.write(data_contents)"
   ]
  },
  {
   "source": [
    "## 0.2 Create a pandas dataframe holding the dataset\n",
    "\n",
    "The two most important data structures in this notebook are the original dataset, created below, and the modified dataset that's created in Task 4 (Fair implementation).\n",
    "\n",
    "The original dataset will be split into training and testing data, the training data will be used to create a modified (fair) dataset later whilst the testing data will not be used except for evaluating models trained on either the original training data or the fair training data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data read in and column names applied\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data = pd.read_csv('../german-credit-dataset/german.csv')\n",
    "data = pd.read_csv('../german-credit-dataset/german-numeric.csv', header=None)\n",
    "data.columns = [\n",
    "    'A1',\n",
    "    'A2',\n",
    "    'A3',\n",
    "    'A5*',\n",
    "    'A6',\n",
    "    'A7',\n",
    "    'A9',\n",
    "    'A11',\n",
    "    'A12',\n",
    "    'A13',\n",
    "    'A14',\n",
    "    'A16',\n",
    "    'A18',\n",
    "    'A19',\n",
    "    'A20',\n",
    "    'A4????',\n",
    "    'A8',\n",
    "    'A10a',\n",
    "    'A10b',\n",
    "    'A15a',\n",
    "    'A15b',\n",
    "    'A17a',\n",
    "    'A17b',\n",
    "    'A17c',\n",
    "    'label'\n",
    "]\n",
    "\n",
    "\n",
    "print('data read in and column names applied')"
   ]
  },
  {
   "source": [
    "## 0.3 Encode the age data as Young (0) and Aged (1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.A13 <= 25, \"A13\"] = 0\n",
    "data.loc[data.A13 > 25, \"A13\"] = 1"
   ]
  },
  {
   "source": [
    "## 3.3.2 Split the data into Features and labels and into training and testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data.iloc[:, :24] # columns 0 to 24\n",
    "labels = data.iloc[:, 24] # column 25\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=0) # This also shuffles the data\n",
    "\n",
    "# print(features.head)\n",
    "\n",
    "# print(labels.head)"
   ]
  },
  {
   "source": [
    "## 3.3.3a Train a Naive Bayes model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "source": [
    "# Import and fit a naive bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "## 3.3.3b Evaluate the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(trained_model, X_test, y_test):\n",
    "    predictions = trained_model.predict(X_test)\n",
    "    print(f'Accuracy: {metrics.accuracy_score(y_test, predictions)}\\n')\n",
    "    print('Classification report:')\n",
    "    print(metrics.classification_report(y_test, predictions, target_names=['Good','Bad'])[:166])\n",
    "    print('Confusion matrix:')\n",
    "    print(metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.7266666666666667\n\nClassification report:\n              precision    recall  f1-score   support\n\n        Good       0.85      0.75      0.80       214\n         Bad       0.52      0.67      0.59        86\n\n  \nConfusion matrix:\n[[160  54]\n [ 28  58]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(nb_classifier, X_test, y_test)"
   ]
  },
  {
   "source": [
    "## 3.3.4 Subsample a new dataset and retrain the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.38666666666666666\n\nClassification report:\n              precision    recall  f1-score   support\n\n        Good       0.77      0.13      0.23       203\n         Bad       0.34      0.92      0.49        97\n\n  \nConfusion matrix:\n[[ 27 176]\n [  8  89]]\n"
     ]
    }
   ],
   "source": [
    "# the aged group has 810 entries, 590 have the positive class\n",
    "# the young group has 190 entries, 110 have the positive class\n",
    "\n",
    "# sampled data should have 95 young +, young -, old +, old -?\n",
    "\n",
    "young_group = data[data['A13'] <= 25]\n",
    "young_pos_group = young_group[young_group['label'] == 1]\n",
    "young_neg_group = young_group[young_group['label'] == 2]\n",
    "\n",
    "aged_group = data[data['A13'] > 25]\n",
    "aged_pos_group = aged_group[aged_group['label'] == 1]\n",
    "aged_neg_group = aged_group[aged_group['label'] == 2]\n",
    "\n",
    "data_resampled = pd.concat([young_pos_group, young_neg_group, aged_pos_group, aged_neg_group])\n",
    "\n",
    "# TODO check that the index gets redone\n",
    "\n",
    "features_new = data_resampled.iloc[:, :24] # columns 0 to 19\n",
    "labels_new = data_resampled.iloc[:, 24] # column 20\n",
    "\n",
    "# print(features_new.head)\n",
    "# print(labels_new.head)\n",
    "\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(features_new, labels_new, test_size=0.3, random_state=0) # This also shuffles the data\n",
    "\n",
    "nb_classifier_resampled_dataset = GaussianNB()\n",
    "nb_classifier_resampled_dataset.fit(X_train_new, y_train_new)\n",
    "evaluate(nb_classifier_resampled_dataset, X_test_new, y_test_new)\n",
    "\n",
    " # The accuracy is now terrible."
   ]
  },
  {
   "source": [
    "# Fairness adjustment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.4.0 Discrimination Measure\n",
    "We use the KCDM measure to test the Discrimination level present within the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.14944769330734242\n"
     ]
    }
   ],
   "source": [
    "def test_discrimination(data):\n",
    "    young_group = data[data['A13'] == 0]\n",
    "    young_pos_group = young_group[young_group['label'] == 1]\n",
    "    aged_group = data[data['A13'] == 1]\n",
    "    aged_pos_group = aged_group[aged_group['label'] == 1]\n",
    "\n",
    "    # print(young_group.shape[0])\n",
    "    # print(young_pos_group.shape[0])\n",
    "    # print(aged_group.shape[0])\n",
    "    # print(aged_pos_group.shape[0])\n",
    "\n",
    "    discrimination = aged_pos_group.shape[0] / aged_group.shape[0] - young_pos_group.shape[0] / young_group.shape[0]\n",
    "    return discrimination\n",
    "\n",
    "print(test_discrimination(data))\n"
   ]
  },
  {
   "source": [
    "## Apply the CND algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The first step is to create a new dataset which is a concatenation of X_train and y_train, this will be modified to become unbiased"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.11223654731080368"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "source": [
    "biased_dataset = pd.concat((X_train, y_train), axis=1)\n",
    "\n",
    "test_discrimination(biased_dataset)\n",
    "\n",
    "# print(new_dataset)\n",
    "\n",
    "# a = new_dataset[new_dataset['label'] == 1]\n",
    "# b = new_dataset[new_dataset['A13'] == 1]\n",
    "# c = a[a['A13'] == 1]\n",
    "\n",
    "# a.shape, b.shape, c.shape"
   ]
  },
  {
   "source": [
    "We want to add a list of label probabilities to this using the pre-built classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(dataset, sensitive_value, sensitive_column, desired_label):\n",
    "    features = dataset.iloc[:, :24] # columns 0 to 24\n",
    "    labels = dataset.iloc[:, 24] # column 25\n",
    "\n",
    "    nb_classifier2 = GaussianNB()\n",
    "    nb_classifier2.fit(features, labels)\n",
    "\n",
    "    def nb_predict(row):\n",
    "        '''\n",
    "        INPUT: A row from the feature data\n",
    "        RETURNS: The probability of that row belonging to the positive class\n",
    "        '''\n",
    "        a = row.values\n",
    "        a = a.reshape(1,-1)\n",
    "        ps = nb_classifier2.predict_proba(a)\n",
    "        return ps[0][0]\n",
    "\n",
    "    # Calculate the probabilities R[x] for x in D and store them in a new column\n",
    "    dataset['rank_score'] = features.apply(nb_predict, axis=1, result_type='expand')\n",
    "    dataset['label'] = labels\n",
    "\n",
    "    # We also add indices for reference\n",
    "    # dataset['new_index'] = range(len(dataset))\n",
    "\n",
    "    candidates_for_promotion = dataset[dataset[sensitive_column] == sensitive_value][dataset['label'] != desired_label]\n",
    "    # print(candidates_for_promotion.shape)\n",
    "    candidates_for_promotion.sort_values('rank_score', inplace=True, ascending=False) # FIXME if it doesn't work swap the ascending value\n",
    "    \n",
    "    candidates_for_demotion = dataset[dataset[sensitive_column] != sensitive_value][dataset['label'] == desired_label]\n",
    "    # print(candidates_for_demotion.shape)\n",
    "    candidates_for_demotion.sort_values('rank_score', inplace=True, ascending=True) # FIXME if it doesn't work swap the ascending value\n",
    "\n",
    "    rest_of_dataset_1 = dataset[dataset[sensitive_column] == sensitive_value][dataset['label'] == desired_label]\n",
    "    rest_of_dataset_2 = dataset[dataset[sensitive_column] != sensitive_value][dataset['label'] != desired_label]\n",
    "    rest_of_dataset = pd.concat([rest_of_dataset_1, rest_of_dataset_2])\n",
    "\n",
    "    return candidates_for_promotion, candidates_for_demotion, rest_of_dataset\n",
    "\n",
    "# rank(new_dataset, 0, 'A13', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "def cnd(dataset, sensitive_value, sensitive_column, desired_label):\n",
    "\n",
    "    candidates_for_promotion, candidates_for_demotion, rest_of_dataset = rank(dataset, sensitive_value, sensitive_column, desired_label)\n",
    "    \n",
    "    # Calculate how many swaps we need\n",
    "    young_group = dataset[dataset[sensitive_column] == sensitive_value]\n",
    "    s = len(young_group)\n",
    "    young_pos_group = young_group[young_group['label'] == desired_label]\n",
    "    s_pos = len(young_pos_group)\n",
    "\n",
    "    aged_group = dataset[dataset[sensitive_column] != sensitive_value]\n",
    "    s_hat = len(aged_group)\n",
    "    aged_pos_group = aged_group[aged_group['label'] == desired_label]\n",
    "    s_hat_pos = len(aged_pos_group)\n",
    "    swaps_required = round(( (s * s_hat_pos) - (s_hat * s_pos) ) / (s + s_hat))\n",
    "    # print(swaps_required)\n",
    "\n",
    "    for i in range(int(swaps_required)):\n",
    "        row_cp = candidates_for_promotion.iloc[[i]]\n",
    "        row_cp['label'] = 1\n",
    "        candidates_for_promotion.iloc[[i]] = row_cp\n",
    "\n",
    "        row_cd = candidates_for_demotion.iloc[[i]]\n",
    "        row_cd['label'] = 2\n",
    "        candidates_for_demotion.iloc[[i]] = row_cd\n",
    "\n",
    "    \n",
    "    new_dataset = pd.concat([rest_of_dataset, candidates_for_promotion, candidates_for_demotion])\n",
    "    return new_dataset\n",
    "\n",
    "unbiased_dataset = cnd(biased_dataset, 0, 'A13', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "-0.00045613705576952324\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "features_u = unbiased_dataset.iloc[:, :24] # columns 0 to 24\n",
    "labels_u = unbiased_dataset.iloc[:, 24] # column 25\n",
    "\n",
    "print(test_discrimination(unbiased_dataset))"
   ]
  },
  {
   "source": [
    "Train a new, unbiased model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "# Import and fit a naive bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_classifier_unbiased = GaussianNB()\n",
    "nb_classifier_unbiased.fit(features_u, labels_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.7233333333333334\n\nClassification report:\n              precision    recall  f1-score   support\n\n        Good       0.80      0.81      0.81       214\n         Bad       0.52      0.50      0.51        86\n\n  \nConfusion matrix:\n[[174  40]\n [ 43  43]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(nb_classifier_unbiased, X_test, y_test)"
   ]
  },
  {
   "source": [
    "# Evaluation and comparison"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}